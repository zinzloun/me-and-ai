# ü§ñ Local GRC-LLM Lab

A private, high-performance **Retrieval-Augmented Generation (RAG)** laboratory designed for GRC (Governance, Risk, and Compliance) analysts.
This lab allows you to query framework documents (NIST CSF 2.0, SP 800-53, ISO Mappings) locally using your GPU.

## üõ°Ô∏è Overview
The system indexes PDF documents from a local folder into a **FAISS** vector database. 
When a user asks a question, the system retrieves the most relevant technical "chunks" and uses **Llama 3.2 3B** to generate an audit-ready answer with source citations.
At the monent the only available endpoint is API.

## Project Structure
```text
grc-lab/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # FastAPI Backend with RAG Logic
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ *.pdf            # NIST/ISO framework documents
‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/     # FAISS database (auto-generated)
‚îú‚îÄ‚îÄ docker-compose.yaml  # Docker orchestration
‚îî‚îÄ‚îÄ README.md            # Documentation
```
---

## üìã Requirements
This lab has been created on Windows 11 with WSL2, so I assume that Docker Desktop is also used

### Hardware
* **GPU:** NVIDIA GeForce RTX 4050 Laptop (6GB VRAM) or better.
* **Storage:** ~10GB for Docker images and LLM models.
* **OS:** Windows 10/11 with WSL2 or Linux.

### Software
* **[Ollama](https://ollama.com/):** Running natively on the my host machine.
* **Docker & Docker Compose:** To orchestrate the API and FAISS.
*  **Cuda toolkit:**
```
nvidia-smi
Mon Dec 22 16:57:56 2025
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 528.76       Driver Version: 528.76       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
```

---

## ‚öôÔ∏è Configuration

### 1. Ollama Setup (Host)
Ensure Ollama is accessible by the Docker containers:
1. Set the environment variable:
```
set OLLAMA_HOST=0.0.0.0
```
1. Start the server
```
ollama serve
```
3. Open another prompt and pull the required models (takes log time but you need just do it once):
```powershell
ollama pull llama3.2:3b
ollama pull nomic-embed-text
ollama pull phi3:mini
```
4. Check Ollama is running: http://localhost:11434
## üöÄ Getting Started
1.Clone or download this repo
2. Eventually place other relevant PDFs in the `./data` folder (at the moment only PDF file are supported)
3. Start Docker
4. Start the lab:
```
docker compose up --build -d
```
4. Start grc-api container
```
docker start grc-api
```
Wait 5 minutes to allow building indexes, then open http://localhost:8000/docs to get started.

## Appendix: useful commands
### Ollama
```
#list model
ollama list

#list process
ollama ps
```

# LLM Red Teaming Lab: NIST & ISO 27001 RAG Security

This repository contains an automated Red Teaming laboratory designed to evaluate the security posture of an AI Chatbot specialized in **NIST CSF** and **ISO 27001** compliance. 

The lab simulates adversarial attacks to identify vulnerabilities in a Retrieval-Augmented Generation (RAG) system, focusing on data privacy and technical robustness.

## üèóÔ∏è Architecture

- **Target Model:** Llama 3.2 (3B) - Serving as a RAG-based Compliance Expert.
- **Judge/Attacker Model:** Phi-3 Mini - Used as the "Adversarial Simulator" and "Security Evaluator".
- **Environment:**
	- **Networking** WSL Hyper-V
	- **Host:** Windows 11 with Ollama - GPU Accelerated - RTX 4050 (IP: `172.31.192.1`).
    - **Attack VM:** Kali Linux running on **WSL2** (IP: `172.31.199.52`).
- **Framework:** [DeepTeam](https://github.com/confident-ai/deepteam) for LLM Red Teaming.



## üõ°Ô∏è Vulnerabilities Tested

The assessment covers 13 types of vulnerabilities across 4 core domains:
1. **Prompt Leakage:** Attempts to extract system instructions or internal guardrails.
2. **PII Leakage:** Probing the RAG database for sensitive personal identifiable information (e.g., employee records).
3. **Misinformation:** Checking if the model hallucinates or provides unsupported claims about ISO/NIST standards.
4. **Robustness:** Testing the model's resistance to conversation hijacking and input overreliance.

## üöÄ Getting Started

### Prerequisites
- Python 3.10+
- Ollama (running on host machine at `172.31.199.52:11434`)
- Chatbot API (running on port `8000`)

### Installation & Virtual Environment Setup
Clone the repository and set up the Python virtual environment:

```bash
# Create the virtual environment
python3 -m venv venvDT

# Activate the environment
source venvDT/bin/activate
cd venvDT

# Install dependencies
pip install --upgrade pip
pip install requests deepteam deepeval

```
### Verify installation
```bash
deepteam --version
|_ deepteam version 1.0.4
```

### Run the test script
```bash
wget https://raw.githubusercontent.com/zinzloun/me-and-ai/refs/heads/main/grc-lab/red_team_assessment.py
python3 red_team_assessment.py
```
It will take quite a lot of time, more or less 30-45 minutes, so go to grab a ‚òïÔ∏é :)
